---v
layout: post
title: "机器学习四之集成学习"
categories: MachineLearning
tags:   datamining machineLearning svm
author: Edward
---

* content
{:toc}

集成学习通过构建并结合多个学习器来完成学习任务，

--------------------

## 一、基本概念

### 1、一般结构

集成学习的一般策略是先产生一组个体学习器，再用某种策略将他们结合起来。

### 2、同质和异质

- 个体学习器都是由同种类型的算法集成的，则这样的集成是同质的，同质中的个体学习器称为基学习器，该算法称为基学习算法
- 集成包含不同类型的基学习器，这样的集成是异质的，个体学习器称为组件学习器

### 3、如何获得好的集成

- 多样性：学习器之间保持一定的差异性，以接近误差相互独立的假设
- 准确性：集成的错误率要小

## 二、集成学习方法

集成学习方法又两种：
- 个体学习器之间存在强依赖关系，必须串行生成的序列化方法，比如Boosting；
- 个体学习器之间不存在强依赖关系，可同时生成的并行化方法，如Bagging和随机森林，这种算法设法使基学习器尽可能具有较大的差异，一个做法是对训练样本进行采样，产生不同的子集。

### 1、Boosting

Boosting是将一族弱学习器提升为强学习器的方法，工作流程是：
- 从原始训练集训练出一个基学习器
- 根据基学习器的表现对训练样本进行调整，使之前基学习器做错的训练样本在后续受到更多关注
- 基于调整后的样本分布来训练下一个基学习器
- 重复进行，直至基学习器数目达到T
- 将这K个基学习器进行加权结合

AdaBoost算法是Boosting算法的一种，推导过程忽略，但过程中需要注意几点：
- 指数损失函数最小化，则分类错误率也将最小化：
![指数损失函数](https://raw.githubusercontent.com/isEdwardTang/Blog/gh-pages/images/exp_loss_function.JPG)
- 根据每次使指数损失最小，可以推导出每次分类器的更新公式：
![分类器权重更新公式](https://raw.githubusercontent.com/isEdwardTang/Blog/gh-pages/images/weight_update.JPG)
- 根据理想的基学习器h<sub>t</sub>将会在分布D<sub>t</sub>下最小化分类误差，将在，推导出样本更新公式如下：
![样本更新公式](https://raw.githubusercontent.com/isEdwardTang/Blog/gh-pages/images/sample_update.JPG)

AdaBoost的算法过程如下：
- 初始化样本权值分布
- 对于学习器h<sub>t</sub>，由样本分布D<sub>t</sub>导出
- 估计h<sub>t</sub>的误差
- 在误差小于0.5的情况下，根据分类器权重更新公式更新权重
- 根据样本更新公式更新样本分布D<sub>t+1</sub>
- 循环上述过程知道t到达指定的T
- 输出基学习器的线性组合：
![加性模型](https://raw.githubusercontent.com/isEdwardTang/Blog/gh-pages/images/adaboost_sign_update.JPG)

Boosting算法的注意点：
- 重赋权值：在训练的每一轮，根据样本分布为每一个训练样本重新赋一个权值
- 重采样法：对于无法接受带权的基学习算法，根据样本分布对训练集重新采样
- 重启动：在抛弃不满足条件的当前基学习器之后，根据当前分布重新对训练样本进行采样
- Boosting算法能基于泛化能力相当弱的学习器构建出很强的集成

### 2、Bagging

Bagging采用的策略是自助采样法(bootstrap sampling)，即有放回的抽样。对于m个样本，进行m次抽样，则原始训练集中约有63.2%的样本出现在采样集中。

Bagging的基本流程是：
- 使用自助采样法，采样T个含m个训练样本的采样集合
- 基于每个采样集训练出对应的基学习器
- 将这些基学习器进行结合
- 对于预测输出，对分类任务使用简单投票法，对于回归任务采用简单平均法，同等票数随机选择一个或者考虑投票的置信度

对于剩下36.8%的样本可以用作验证集来进行包外估计，可用于：
- 基学习器是决策树时，可以使用包外样本来辅助剪枝，或者用于估计决策树中各结点的后验概率以辅助对零训练样本的处理
- 基学习器是神经网络时，可以使用包外样本来辅助早期停止以减少过拟合的风险

### 3、Random Forest

RF是指在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性的选择。
对于传统决策树，在划分属性时是在当前结点的属性集合中选择一个最优属性；
而对于RF，过程是：
- 对于基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集
- 然后从这个子集中选择一个最优属性用于划分
- k控制了随机的程度，k和属性个数d相同时则是传统决策树，k=1是随机选择，一般推荐k=log2d

RF在通过样本扰动的基础上，还进行了属性扰动，进一步增加了个体学习器之间的差异性，使泛化性能进一步提高。和Bagging相比，随着个体学习器的增加，随机森林通常会收敛到更低的泛化误差

## 三、结合策略

结合策略有三个好处：

- 统计方面看，可能有多个学习器达到相同性能，单一学习器会导致误选，结合多个学习器会增强泛化误差
- 计算方面看，学习算法往往会陷入局部最小，多次运行会降低局部最小的风险
- 表示方面看，可能真实假设并不在假设空间内，使用单个学习器一定无效，结合多个学习器，使假设空间扩大，可能达到更高的近似

![结合策略的好处](https://raw.githubusercontent.com/isEdwardTang/Blog/gh-pages/images/ensemble_learning_positive.JPG)

### 1、平均法

包括简单学习法和加权平均法。不同的集成学习方法，可以认为是通过不同的方法来确定加权平均法中的基学习器权重。对于个体学习器性能相差较大时宜使用加权平均，而在个体学习器性能相近时宜使用简单平均。

### 2、投票法

分类任务通常采用投票法。

- 绝大多数投票法：即若某标记得票过半数，则该预测为该标记，否则拒绝预测
- 相对多数投票法：即预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个
- 加权投票法：与加权平均法类似

绝大多数投票法提供了拒绝预测的选项，如果学习任务必须要求预测结果，则绝对多数投票法将退化为相对多数投票法。

个体学习器输出值的类型类标记和类概率，其投票分别叫做硬投票和软投票。

### 3、学习法

学习法是指用一个专门的学习器用于结合。Stacking是著名的一种，将个体学习器称为初级学习器，将用于结合的学习器称为次级学习器或元学习器。
Stacking算法过程如下：
- 从初级数据集训练出初级学习器
- 然后生成一个新数据集用于训练次级学习器

将初级学习器的输出类概率作为次级学习器的输入属性，使用多响应线性回归(Multi-response Linear Regression)MLR作为次级学习器学习算法较好。

## 四、多样性

### 1、误差-分歧分解

误差-分歧分解对回归学习进行集成学习器的性能推导，得出集成的泛化误差 = 个体学习器的泛化误差的加权平均 - 个体学习器的加权平均值，即个体学习器准确度越高，多样性越大，则集成越好。

### 2、多样性度量

考虑个体分类器的两两相似/不相似度，对于二分类任务，a表示两个学习器均预测为正，b表示一正一负，c表示一负一正，d表示两个均为负，a+b+c+d=m，一些多样性度量如下：

- 不合度量(disagreement measure):

![不合度量](https://raw.githubusercontent.com/isEdwardTang/Blog/gh-pages/images/disagreement_measure.JPG)

值域为[0,1]，值越大多样性越大

- 相关系数(correlation coefficient)：

![相关系数](https://raw.githubusercontent.com/isEdwardTang/Blog/gh-pages/images/correlation_coefficient.JPG)

值域为[-1,1]，为0时表示无关，正数表示正相关，负数表示负相关

- Q-统计量(Q-statistic)：

![Q-统计量](https://raw.githubusercontent.com/isEdwardTang/Blog/gh-pages/images/Q-statistic.JPG)

与相关系数符号相同，只是绝对值更大

- &kappa;-统计量(&kappa;-statistic)：

![&kappa;-统计量](https://raw.githubusercontent.com/isEdwardTang/Blog/gh-pages/images/k-statistic.JPG)

p1是两个分类器取得一致的概率，p2是两个分类器偶然达成一致的概率。
1表示两个分类器完全一致，0表示偶然一致。

### 3、多样性增强

为了增强多样性，一般在学习过程中引入随机性，常见做法是对数据样本、输入属性、输出表示、算法参数进行扰动

- 数据样本扰动：

从给定的数据中选择不同的数据子集，分别训练出不同的个体学习器。常常基于采样法。对于不稳定基学习器很有效，比如决策树、神经网络；但是某些对数据样本的扰动不敏感，例如线性学习器，支持向量机、朴素贝叶斯、k近邻等，这种方法不使用

- 输入属性扰动：

随机子空间算法每次从初始属性集中抽取若干属性子集，再基于每个属性子集训练一个基学习器。对于属性较少，或冗余属性少，不宜使用

- 输出表示扰动：

对输出表示进行操纵，如：
    - 翻转法：对训练样本的类标记稍作变动
    - 输出调制法：对输出表示进行转化，如将分类输出转化为回归输出
    - EOOC法：将原任务拆解为多个可同时求解的任务

- 算法参数扰动：调整基学习算法的参数

