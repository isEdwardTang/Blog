---
layout: post
title: "机器学习七之降维与度量学习"
categories: MachineLearning
tags:   datamining machineLearning dimensionReduction
author: Edward
---

* content
{:toc}

--------------------

## 一、降维基础

### 1、k-近邻学习

k近邻学习(k-Nearest Neighbor，简称kNN)算法是给定测试样本，基于某种距离计算找出训练集里面与其最靠近的k个训练样本，然后基于这k个邻居的信息来进行预测。通常，对于分类任务采取投票法，即选择这k个样本中出现最多的类别标记作为预测结果；而在回归任务中采取平均法，将这k个样本的实值输出标记的平均或加权平均作为预测。

k近邻学习是一种懒惰学习(lazy learning)，训练开销为0；和其对应的是急切学习(eager learning)。

- 基于任意样本x附近任意小的&delta;距离范围内总能找到一个测试样本，即训练样本的采样密度足够大，我们可以得出虽然邻分类器简单，但其泛化错误率不超过贝叶斯最优分类器的错误率的两倍。

### 2、低维嵌入

对于k近邻学习，当属性值较多时，想要满足&delta;距离，则需要的样本密度会很大。

高维情况称之为维数灾难(Curse of dimensionality)，缓解维数灾难的一个重要途径是降维(dimension reduction)，即通过数学变换将原始高维属性空间转变为一个低维子空间，而我们训练的其实是高维空间下的一个低维嵌入。

要使原始空间中样本之间的距离在低维空间中得以保持，可使用多维缩放(Multiple Dimensional Scaling，简称MDS)这种降维方法。设原始距离矩阵为D，降维后的d<sup>'</sup>维空间中的为Z。过程如下：

- 计算降维后的样本内积矩阵B=Z<sup>T</sup>Z，其中b<sub>ij</sub>=z<sub>i</sub><sup>T</sup>z<sub>j</sub>，按照如下公式计算：

![内积矩阵计算]()

- 对矩阵B进行特征值分解
- 取&Lambda;<sup>&tilde;</sup>表示d<sup>'</sup个最大特征值所构成的对角矩阵， V<sup>&tilde;</sup>为相应的特征向量矩阵
- 则矩阵V<sup>&tilde;</sup>&Lambda;<sup>&tilde;1/2</sup的每行是一个样本的低维坐标

### 3、线性降维

Z=W<sup>T</sup>X，其中X为原空间中的样本，W为变换矩阵，Z为新空间。这种降维方式称线性降维，因为新空间中的属性是原空间属性的线性组合。

## 二、降维算法

### 1、主成分分析

主成分分析(Principal Component Analysis,PCA)基于最近重构性和最大可分性：
- 最近重构性：样本点到某个超平面的距离都足够近
- 最大可分性：样本点在这个超平面上的投影都能尽量分开
那么这个超平面可以对正交属性空间中的所有样本进行恰当的表达。

过程如下：
- 对所有样本进行中心化：

![样本中心化]()

- 计算样本的协方差矩阵XX<sup>T</sup>
- 对协方差矩阵进行特征值分解
- 取最大的d<sup>'</sup>个特征值所对应的特征向量

对于降维后低维空间的维数的确定，有如下方式：
- 直接指定
- 通过不同低维空间中对k近邻分类器进行交叉验证来选取
- 对于PCA，可以指定一个阈值t，选取使t最小的维数值：

![重构阈值]()

### 2、核化线性降维

线性降维方式可能丢失低维结构，而非线性降维可以保留本真空间，常见的算法有核主成分分析(Kernelized PCA，简称KPCA)

### 3、流形学习

### 4、度量学习
